{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, array_to_img\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv').drop(columns=['Id'])\n",
    "X = np.array(df.iloc[:, 1:])\n",
    "y = to_categorical(np.array(df.iloc[:, 0]))\n",
    "\n",
    "# Convert the training and test images into 3 channels\n",
    "X = np.dstack([X] * 3)\n",
    "# Reshape images as per the tensor format required by tensorflow\n",
    "X = X.reshape(-1, 28,28,3)\n",
    "X = np.asarray([img_to_array(array_to_img(im, scale=False).resize((150,150))) for im in X])\n",
    "X = X.astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = 0.5, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/50\n",
      "48000/48000 [==============================] - 49s 1ms/sample - loss: 1.2208 - accuracy: 0.5310 - val_loss: 1.8553 - val_accuracy: 0.4417\n",
      "Epoch 2/50\n",
      "48000/48000 [==============================] - 42s 873us/sample - loss: 0.8428 - accuracy: 0.6455 - val_loss: 2.2835 - val_accuracy: 0.4398\n",
      "Epoch 3/50\n",
      "48000/48000 [==============================] - 42s 870us/sample - loss: 0.7434 - accuracy: 0.6875 - val_loss: 3.7044 - val_accuracy: 0.3965\n",
      "Epoch 4/50\n",
      "48000/48000 [==============================] - 41s 855us/sample - loss: 0.6751 - accuracy: 0.7222 - val_loss: 3.1960 - val_accuracy: 0.4030\n",
      "Epoch 5/50\n",
      "48000/48000 [==============================] - 41s 857us/sample - loss: 0.6124 - accuracy: 0.7477 - val_loss: 3.6605 - val_accuracy: 0.3892\n",
      "Epoch 6/50\n",
      "48000/48000 [==============================] - 41s 854us/sample - loss: 0.5573 - accuracy: 0.7761 - val_loss: 3.8451 - val_accuracy: 0.4103\n",
      "Epoch 7/50\n",
      "47872/48000 [============================>.] - ETA: 0s - loss: 0.4975 - accuracy: 0.7992"
     ]
    }
   ],
   "source": [
    "base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "for fc in [1024, 512, 256]:\n",
    "    x = Dense(fc, activation='relu')(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "# x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(5, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=30, verbose=1, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_2\n",
      "1 conv2d_94\n",
      "2 batch_normalization_94\n",
      "3 activation_94\n",
      "4 conv2d_95\n",
      "5 batch_normalization_95\n",
      "6 activation_95\n",
      "7 conv2d_96\n",
      "8 batch_normalization_96\n",
      "9 activation_96\n",
      "10 max_pooling2d_4\n",
      "11 conv2d_97\n",
      "12 batch_normalization_97\n",
      "13 activation_97\n",
      "14 conv2d_98\n",
      "15 batch_normalization_98\n",
      "16 activation_98\n",
      "17 max_pooling2d_5\n",
      "18 conv2d_102\n",
      "19 batch_normalization_102\n",
      "20 activation_102\n",
      "21 conv2d_100\n",
      "22 conv2d_103\n",
      "23 batch_normalization_100\n",
      "24 batch_normalization_103\n",
      "25 activation_100\n",
      "26 activation_103\n",
      "27 average_pooling2d_9\n",
      "28 conv2d_99\n",
      "29 conv2d_101\n",
      "30 conv2d_104\n",
      "31 conv2d_105\n",
      "32 batch_normalization_99\n",
      "33 batch_normalization_101\n",
      "34 batch_normalization_104\n",
      "35 batch_normalization_105\n",
      "36 activation_99\n",
      "37 activation_101\n",
      "38 activation_104\n",
      "39 activation_105\n",
      "40 mixed0\n",
      "41 conv2d_109\n",
      "42 batch_normalization_109\n",
      "43 activation_109\n",
      "44 conv2d_107\n",
      "45 conv2d_110\n",
      "46 batch_normalization_107\n",
      "47 batch_normalization_110\n",
      "48 activation_107\n",
      "49 activation_110\n",
      "50 average_pooling2d_10\n",
      "51 conv2d_106\n",
      "52 conv2d_108\n",
      "53 conv2d_111\n",
      "54 conv2d_112\n",
      "55 batch_normalization_106\n",
      "56 batch_normalization_108\n",
      "57 batch_normalization_111\n",
      "58 batch_normalization_112\n",
      "59 activation_106\n",
      "60 activation_108\n",
      "61 activation_111\n",
      "62 activation_112\n",
      "63 mixed1\n",
      "64 conv2d_116\n",
      "65 batch_normalization_116\n",
      "66 activation_116\n",
      "67 conv2d_114\n",
      "68 conv2d_117\n",
      "69 batch_normalization_114\n",
      "70 batch_normalization_117\n",
      "71 activation_114\n",
      "72 activation_117\n",
      "73 average_pooling2d_11\n",
      "74 conv2d_113\n",
      "75 conv2d_115\n",
      "76 conv2d_118\n",
      "77 conv2d_119\n",
      "78 batch_normalization_113\n",
      "79 batch_normalization_115\n",
      "80 batch_normalization_118\n",
      "81 batch_normalization_119\n",
      "82 activation_113\n",
      "83 activation_115\n",
      "84 activation_118\n",
      "85 activation_119\n",
      "86 mixed2\n",
      "87 conv2d_121\n",
      "88 batch_normalization_121\n",
      "89 activation_121\n",
      "90 conv2d_122\n",
      "91 batch_normalization_122\n",
      "92 activation_122\n",
      "93 conv2d_120\n",
      "94 conv2d_123\n",
      "95 batch_normalization_120\n",
      "96 batch_normalization_123\n",
      "97 activation_120\n",
      "98 activation_123\n",
      "99 max_pooling2d_6\n",
      "100 mixed3\n",
      "101 conv2d_128\n",
      "102 batch_normalization_128\n",
      "103 activation_128\n",
      "104 conv2d_129\n",
      "105 batch_normalization_129\n",
      "106 activation_129\n",
      "107 conv2d_125\n",
      "108 conv2d_130\n",
      "109 batch_normalization_125\n",
      "110 batch_normalization_130\n",
      "111 activation_125\n",
      "112 activation_130\n",
      "113 conv2d_126\n",
      "114 conv2d_131\n",
      "115 batch_normalization_126\n",
      "116 batch_normalization_131\n",
      "117 activation_126\n",
      "118 activation_131\n",
      "119 average_pooling2d_12\n",
      "120 conv2d_124\n",
      "121 conv2d_127\n",
      "122 conv2d_132\n",
      "123 conv2d_133\n",
      "124 batch_normalization_124\n",
      "125 batch_normalization_127\n",
      "126 batch_normalization_132\n",
      "127 batch_normalization_133\n",
      "128 activation_124\n",
      "129 activation_127\n",
      "130 activation_132\n",
      "131 activation_133\n",
      "132 mixed4\n",
      "133 conv2d_138\n",
      "134 batch_normalization_138\n",
      "135 activation_138\n",
      "136 conv2d_139\n",
      "137 batch_normalization_139\n",
      "138 activation_139\n",
      "139 conv2d_135\n",
      "140 conv2d_140\n",
      "141 batch_normalization_135\n",
      "142 batch_normalization_140\n",
      "143 activation_135\n",
      "144 activation_140\n",
      "145 conv2d_136\n",
      "146 conv2d_141\n",
      "147 batch_normalization_136\n",
      "148 batch_normalization_141\n",
      "149 activation_136\n",
      "150 activation_141\n",
      "151 average_pooling2d_13\n",
      "152 conv2d_134\n",
      "153 conv2d_137\n",
      "154 conv2d_142\n",
      "155 conv2d_143\n",
      "156 batch_normalization_134\n",
      "157 batch_normalization_137\n",
      "158 batch_normalization_142\n",
      "159 batch_normalization_143\n",
      "160 activation_134\n",
      "161 activation_137\n",
      "162 activation_142\n",
      "163 activation_143\n",
      "164 mixed5\n",
      "165 conv2d_148\n",
      "166 batch_normalization_148\n",
      "167 activation_148\n",
      "168 conv2d_149\n",
      "169 batch_normalization_149\n",
      "170 activation_149\n",
      "171 conv2d_145\n",
      "172 conv2d_150\n",
      "173 batch_normalization_145\n",
      "174 batch_normalization_150\n",
      "175 activation_145\n",
      "176 activation_150\n",
      "177 conv2d_146\n",
      "178 conv2d_151\n",
      "179 batch_normalization_146\n",
      "180 batch_normalization_151\n",
      "181 activation_146\n",
      "182 activation_151\n",
      "183 average_pooling2d_14\n",
      "184 conv2d_144\n",
      "185 conv2d_147\n",
      "186 conv2d_152\n",
      "187 conv2d_153\n",
      "188 batch_normalization_144\n",
      "189 batch_normalization_147\n",
      "190 batch_normalization_152\n",
      "191 batch_normalization_153\n",
      "192 activation_144\n",
      "193 activation_147\n",
      "194 activation_152\n",
      "195 activation_153\n",
      "196 mixed6\n",
      "197 conv2d_158\n",
      "198 batch_normalization_158\n",
      "199 activation_158\n",
      "200 conv2d_159\n",
      "201 batch_normalization_159\n",
      "202 activation_159\n",
      "203 conv2d_155\n",
      "204 conv2d_160\n",
      "205 batch_normalization_155\n",
      "206 batch_normalization_160\n",
      "207 activation_155\n",
      "208 activation_160\n",
      "209 conv2d_156\n",
      "210 conv2d_161\n",
      "211 batch_normalization_156\n",
      "212 batch_normalization_161\n",
      "213 activation_156\n",
      "214 activation_161\n",
      "215 average_pooling2d_15\n",
      "216 conv2d_154\n",
      "217 conv2d_157\n",
      "218 conv2d_162\n",
      "219 conv2d_163\n",
      "220 batch_normalization_154\n",
      "221 batch_normalization_157\n",
      "222 batch_normalization_162\n",
      "223 batch_normalization_163\n",
      "224 activation_154\n",
      "225 activation_157\n",
      "226 activation_162\n",
      "227 activation_163\n",
      "228 mixed7\n",
      "229 conv2d_166\n",
      "230 batch_normalization_166\n",
      "231 activation_166\n",
      "232 conv2d_167\n",
      "233 batch_normalization_167\n",
      "234 activation_167\n",
      "235 conv2d_164\n",
      "236 conv2d_168\n",
      "237 batch_normalization_164\n",
      "238 batch_normalization_168\n",
      "239 activation_164\n",
      "240 activation_168\n",
      "241 conv2d_165\n",
      "242 conv2d_169\n",
      "243 batch_normalization_165\n",
      "244 batch_normalization_169\n",
      "245 activation_165\n",
      "246 activation_169\n",
      "247 max_pooling2d_7\n",
      "248 mixed8\n",
      "249 conv2d_174\n",
      "250 batch_normalization_174\n",
      "251 activation_174\n",
      "252 conv2d_171\n",
      "253 conv2d_175\n",
      "254 batch_normalization_171\n",
      "255 batch_normalization_175\n",
      "256 activation_171\n",
      "257 activation_175\n",
      "258 conv2d_172\n",
      "259 conv2d_173\n",
      "260 conv2d_176\n",
      "261 conv2d_177\n",
      "262 average_pooling2d_16\n",
      "263 conv2d_170\n",
      "264 batch_normalization_172\n",
      "265 batch_normalization_173\n",
      "266 batch_normalization_176\n",
      "267 batch_normalization_177\n",
      "268 conv2d_178\n",
      "269 batch_normalization_170\n",
      "270 activation_172\n",
      "271 activation_173\n",
      "272 activation_176\n",
      "273 activation_177\n",
      "274 batch_normalization_178\n",
      "275 activation_170\n",
      "276 mixed9_0\n",
      "277 concatenate_2\n",
      "278 activation_178\n",
      "279 mixed9\n",
      "280 conv2d_183\n",
      "281 batch_normalization_183\n",
      "282 activation_183\n",
      "283 conv2d_180\n",
      "284 conv2d_184\n",
      "285 batch_normalization_180\n",
      "286 batch_normalization_184\n",
      "287 activation_180\n",
      "288 activation_184\n",
      "289 conv2d_181\n",
      "290 conv2d_182\n",
      "291 conv2d_185\n",
      "292 conv2d_186\n",
      "293 average_pooling2d_17\n",
      "294 conv2d_179\n",
      "295 batch_normalization_181\n",
      "296 batch_normalization_182\n",
      "297 batch_normalization_185\n",
      "298 batch_normalization_186\n",
      "299 conv2d_187\n",
      "300 batch_normalization_179\n",
      "301 activation_181\n",
      "302 activation_182\n",
      "303 activation_185\n",
      "304 activation_186\n",
      "305 batch_normalization_187\n",
      "306 activation_179\n",
      "307 mixed9_1\n",
      "308 concatenate_3\n",
      "309 activation_187\n",
      "310 mixed10\n"
     ]
    }
   ],
   "source": [
    "for i, layer in enumerate(base_model.layers):\n",
    "   print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 6000 samples\n",
      "Epoch 1/50\n",
      "48000/48000 [==============================] - 49s 1ms/sample - loss: 0.0762 - val_loss: 15.1119\n",
      "Epoch 2/50\n",
      "48000/48000 [==============================] - 42s 879us/sample - loss: 0.0442 - val_loss: 14.9987\n",
      "Epoch 3/50\n",
      "48000/48000 [==============================] - 43s 900us/sample - loss: 0.0387 - val_loss: 15.0123\n",
      "Epoch 4/50\n",
      "48000/48000 [==============================] - 45s 929us/sample - loss: 0.0339 - val_loss: 14.9877\n",
      "Epoch 5/50\n",
      "48000/48000 [==============================] - 42s 882us/sample - loss: 0.0293 - val_loss: 15.0370\n",
      "Epoch 6/50\n",
      "48000/48000 [==============================] - 44s 913us/sample - loss: 0.0274 - val_loss: 14.6454\n",
      "Epoch 7/50\n",
      "48000/48000 [==============================] - 44s 927us/sample - loss: 0.0244 - val_loss: 14.8529\n",
      "Epoch 8/50\n",
      "48000/48000 [==============================] - 43s 889us/sample - loss: 0.0254 - val_loss: 14.6497\n",
      "Epoch 9/50\n",
      "48000/48000 [==============================] - 44s 919us/sample - loss: 0.0224 - val_loss: 14.4782\n",
      "Epoch 10/50\n",
      "48000/48000 [==============================] - 43s 906us/sample - loss: 0.0191 - val_loss: 14.3255\n",
      "Epoch 11/50\n",
      "48000/48000 [==============================] - 42s 874us/sample - loss: 0.0195 - val_loss: 14.4745\n",
      "Epoch 12/50\n",
      "48000/48000 [==============================] - 43s 901us/sample - loss: 0.0165 - val_loss: 14.4976\n",
      "Epoch 13/50\n",
      "48000/48000 [==============================] - 44s 923us/sample - loss: 0.0158 - val_loss: 14.5727\n",
      "Epoch 14/50\n",
      "48000/48000 [==============================] - 43s 906us/sample - loss: 0.0185 - val_loss: 14.5264\n",
      "Epoch 15/50\n",
      "48000/48000 [==============================] - 44s 923us/sample - loss: 0.0169 - val_loss: 14.7595\n",
      "Epoch 16/50\n",
      "48000/48000 [==============================] - 42s 879us/sample - loss: 0.0139 - val_loss: 14.9521\n",
      "Epoch 17/50\n",
      "48000/48000 [==============================] - 43s 892us/sample - loss: 0.0137 - val_loss: 14.8657\n",
      "Epoch 18/50\n",
      "48000/48000 [==============================] - 42s 876us/sample - loss: 0.0150 - val_loss: 14.8552\n",
      "Epoch 19/50\n",
      "48000/48000 [==============================] - 42s 866us/sample - loss: 0.0123 - val_loss: 14.6766\n",
      "Epoch 20/50\n",
      "48000/48000 [==============================] - 43s 888us/sample - loss: 0.0130 - val_loss: 14.4442\n",
      "Epoch 21/50\n",
      "48000/48000 [==============================] - 43s 900us/sample - loss: 0.0125 - val_loss: 14.4803\n",
      "Epoch 22/50\n",
      "48000/48000 [==============================] - 42s 880us/sample - loss: 0.0126 - val_loss: 14.5714\n",
      "Epoch 23/50\n",
      "48000/48000 [==============================] - 41s 862us/sample - loss: 0.0121 - val_loss: 14.5883\n",
      "Epoch 24/50\n",
      "48000/48000 [==============================] - 43s 887us/sample - loss: 0.0116 - val_loss: 14.7134\n",
      "Epoch 25/50\n",
      "48000/48000 [==============================] - 42s 881us/sample - loss: 0.0111 - val_loss: 14.8455\n",
      "Epoch 26/50\n",
      "48000/48000 [==============================] - 42s 876us/sample - loss: 0.0111 - val_loss: 14.8588\n",
      "Epoch 27/50\n",
      "48000/48000 [==============================] - 43s 901us/sample - loss: 0.0132 - val_loss: 14.8275\n",
      "Epoch 28/50\n",
      "48000/48000 [==============================] - 43s 894us/sample - loss: 0.0119 - val_loss: 14.7493\n",
      "Epoch 29/50\n",
      "48000/48000 [==============================] - 43s 897us/sample - loss: 0.0097 - val_loss: 14.7361\n",
      "Epoch 30/50\n",
      "48000/48000 [==============================] - 42s 877us/sample - loss: 0.0099 - val_loss: 14.7137\n",
      "Epoch 31/50\n",
      "48000/48000 [==============================] - 44s 907us/sample - loss: 0.0104 - val_loss: 14.6117\n",
      "Epoch 32/50\n",
      "48000/48000 [==============================] - 43s 892us/sample - loss: 0.0098 - val_loss: 14.5172\n",
      "Epoch 33/50\n",
      "48000/48000 [==============================] - 42s 870us/sample - loss: 0.0095 - val_loss: 14.6034\n",
      "Epoch 34/50\n",
      "48000/48000 [==============================] - 42s 874us/sample - loss: 0.0088 - val_loss: 14.5141\n",
      "Epoch 35/50\n",
      "48000/48000 [==============================] - 42s 880us/sample - loss: 0.0079 - val_loss: 14.5287\n",
      "Epoch 36/50\n",
      "48000/48000 [==============================] - 41s 851us/sample - loss: 0.0081 - val_loss: 14.5390\n",
      "Epoch 37/50\n",
      "48000/48000 [==============================] - 43s 893us/sample - loss: 0.0103 - val_loss: 14.4945\n",
      "Epoch 38/50\n",
      "48000/48000 [==============================] - 42s 869us/sample - loss: 0.0077 - val_loss: 14.5011\n",
      "Epoch 39/50\n",
      "48000/48000 [==============================] - 41s 860us/sample - loss: 0.0094 - val_loss: 14.5006\n",
      "Epoch 40/50\n",
      "48000/48000 [==============================] - 42s 872us/sample - loss: 0.0080 - val_loss: 14.4584\n",
      "Epoch 41/50\n",
      "48000/48000 [==============================] - 41s 864us/sample - loss: 0.0080 - val_loss: 14.4972\n",
      "Epoch 42/50\n",
      "48000/48000 [==============================] - 42s 871us/sample - loss: 0.0097 - val_loss: 14.6280\n",
      "Epoch 43/50\n",
      "48000/48000 [==============================] - 43s 898us/sample - loss: 0.0077 - val_loss: 14.5867\n",
      "Epoch 44/50\n",
      "48000/48000 [==============================] - 41s 864us/sample - loss: 0.0065 - val_loss: 14.5187\n",
      "Epoch 45/50\n",
      "48000/48000 [==============================] - 42s 869us/sample - loss: 0.0074 - val_loss: 14.5714\n",
      "Epoch 46/50\n",
      "48000/48000 [==============================] - 42s 876us/sample - loss: 0.0089 - val_loss: 14.5964\n",
      "Epoch 47/50\n",
      "48000/48000 [==============================] - 41s 860us/sample - loss: 0.0066 - val_loss: 14.6520\n",
      "Epoch 48/50\n",
      "48000/48000 [==============================] - 41s 855us/sample - loss: 0.0062 - val_loss: 14.6244\n",
      "Epoch 49/50\n",
      "48000/48000 [==============================] - 41s 863us/sample - loss: 0.0073 - val_loss: 14.6039\n",
      "Epoch 50/50\n",
      "48000/48000 [==============================] - 42s 881us/sample - loss: 0.0067 - val_loss: 14.5102\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe9501ac350>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for layer in model.layers[:288]:\n",
    "   layer.trainable = False\n",
    "for layer in model.layers[288:]:\n",
    "   layer.trainable = True\n",
    "\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.fit(X_train, y_train, batch_size=64, epochs=30, verbose=1, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 14.827917289733886\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
