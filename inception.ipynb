{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPool2D, GlobalMaxPooling2D\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.imagenet_utils import preprocess_input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, array_to_img\n",
    "from tensorflow.keras.layers.merge import concatenate\n",
    "import tensorflow.keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "K.set_learning_phase(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inception_module(layer_in, f1, f2_in, f2_out, f3_in, f3_out, f4_out):\n",
    "    conv1 = Conv2D(f1, (1,1), padding='same', activation='relu')(layer_in)\n",
    "    conv3 = Conv2D(f2_in, (1,1), padding='same', activation='relu')(layer_in)\n",
    "    conv3 = Conv2D(f2_out, (3,3), padding='same', activation='relu')(conv3)\n",
    "    conv5 = Conv2D(f3_in, (1,1), padding='same', activation='relu')(layer_in)\n",
    "    conv5 = Conv2D(f3_out, (5,5), padding='same', activation='relu')(conv5)\n",
    "    pool = MaxPooling2D((3,3), strides=(1,1), padding='same')(layer_in)\n",
    "    pool = Conv2D(f4_out, (1,1), padding='same', activation='relu')(pool)\n",
    "    layer_out = concatenate([conv1, conv3, conv5, pool], axis=-1)\n",
    "    return layer_out\n",
    " \n",
    "# define model input\n",
    "visible = Input(shape=(150, 150, 3))\n",
    "# add inception block 1\n",
    "layer = inception_module(visible, 64, 96, 128, 16, 32, 32)\n",
    "# add inception block 1\n",
    "layer = inception_module(layer, 128, 128, 192, 32, 96, 64)\n",
    "\n",
    "for fc in [1024, 512, 256]:\n",
    "    layer = Dense(fc, activation='relu')(layer)\n",
    "    layer = Dropout(0.5)(layer)\n",
    "predictions = Dense(5, activation='softmax')(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=visible, outputs=predictions)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv').drop(columns=['Id'])\n",
    "X = np.array(df.iloc[:, 1:])\n",
    "y = to_categorical(np.array(df.iloc[:, 0]))\n",
    "\n",
    "# Convert the training and test images into 3 channels\n",
    "X = np.dstack([X] * 3)\n",
    "# Reshape images as per the tensor format required by tensorflow\n",
    "X = X.reshape(-1, 28, 28, 3)\n",
    "X = np.asarray([img_to_array(array_to_img(im, scale=False).resize((150,150))) for im in X])\n",
    "X = X.astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 64, 64, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 6000 samples\n",
      "Epoch 1/25\n",
      "48000/48000 [==============================] - 46s 952us/sample - loss: 0.6341 - accuracy: 0.7706 - val_loss: 49.8181 - val_accuracy: 0.6758\n",
      "Epoch 2/25\n",
      "48000/48000 [==============================] - 34s 718us/sample - loss: 0.4306 - accuracy: 0.8296 - val_loss: 0.3576 - val_accuracy: 0.8590\n",
      "Epoch 3/25\n",
      "48000/48000 [==============================] - 35s 723us/sample - loss: 0.3535 - accuracy: 0.8574 - val_loss: 0.5823 - val_accuracy: 0.7727\n",
      "Epoch 4/25\n",
      "48000/48000 [==============================] - 35s 726us/sample - loss: 0.3280 - accuracy: 0.8677 - val_loss: 0.3106 - val_accuracy: 0.8778\n",
      "Epoch 5/25\n",
      "48000/48000 [==============================] - 35s 727us/sample - loss: 0.2984 - accuracy: 0.8780 - val_loss: 0.3533 - val_accuracy: 0.8610\n",
      "Epoch 6/25\n",
      "48000/48000 [==============================] - 35s 723us/sample - loss: 0.2818 - accuracy: 0.8856 - val_loss: 0.4454 - val_accuracy: 0.8302\n",
      "Epoch 7/25\n",
      "48000/48000 [==============================] - 35s 727us/sample - loss: 0.4245 - accuracy: 0.8390 - val_loss: 24.8276 - val_accuracy: 0.5235\n",
      "Epoch 8/25\n",
      "48000/48000 [==============================] - 35s 726us/sample - loss: 0.3818 - accuracy: 0.8502 - val_loss: 0.3407 - val_accuracy: 0.8615\n",
      "Epoch 9/25\n",
      "48000/48000 [==============================] - 35s 724us/sample - loss: 0.3146 - accuracy: 0.8694 - val_loss: 0.3732 - val_accuracy: 0.8488\n",
      "Epoch 10/25\n",
      "48000/48000 [==============================] - 35s 731us/sample - loss: 0.3970 - accuracy: 0.8416 - val_loss: 0.4415 - val_accuracy: 0.8170\n",
      "Epoch 11/25\n",
      "48000/48000 [==============================] - 35s 723us/sample - loss: 0.3639 - accuracy: 0.8536 - val_loss: 187.8167 - val_accuracy: 0.7045\n",
      "Epoch 12/25\n",
      "48000/48000 [==============================] - 35s 721us/sample - loss: 0.3275 - accuracy: 0.8683 - val_loss: 0.3020 - val_accuracy: 0.8848\n",
      "Epoch 13/25\n",
      "48000/48000 [==============================] - 35s 725us/sample - loss: 0.2912 - accuracy: 0.8822 - val_loss: 0.4849 - val_accuracy: 0.8120\n",
      "Epoch 14/25\n",
      "48000/48000 [==============================] - 35s 720us/sample - loss: 0.2644 - accuracy: 0.8916 - val_loss: 0.4206 - val_accuracy: 0.8378\n",
      "Epoch 15/25\n",
      "48000/48000 [==============================] - 35s 726us/sample - loss: 0.2445 - accuracy: 0.9004 - val_loss: 0.3101 - val_accuracy: 0.8748\n",
      "Epoch 16/25\n",
      "48000/48000 [==============================] - 35s 722us/sample - loss: 0.2504 - accuracy: 0.8976 - val_loss: 0.3193 - val_accuracy: 0.8690\n",
      "Epoch 17/25\n",
      "48000/48000 [==============================] - 35s 724us/sample - loss: 0.2276 - accuracy: 0.9065 - val_loss: 0.3054 - val_accuracy: 0.8775\n",
      "Epoch 18/25\n",
      "48000/48000 [==============================] - 35s 720us/sample - loss: 0.2987 - accuracy: 0.8877 - val_loss: 0.3123 - val_accuracy: 0.8743\n",
      "Epoch 19/25\n",
      "48000/48000 [==============================] - 35s 726us/sample - loss: 0.2214 - accuracy: 0.9100 - val_loss: 0.3331 - val_accuracy: 0.8680\n",
      "Epoch 20/25\n",
      "48000/48000 [==============================] - 35s 723us/sample - loss: 0.2022 - accuracy: 0.9156 - val_loss: 0.3084 - val_accuracy: 0.8802\n",
      "Epoch 21/25\n",
      "48000/48000 [==============================] - 35s 721us/sample - loss: 0.1870 - accuracy: 0.9229 - val_loss: 0.3411 - val_accuracy: 0.8742\n",
      "Epoch 22/25\n",
      "48000/48000 [==============================] - 35s 725us/sample - loss: 0.1838 - accuracy: 0.9252 - val_loss: 0.4264 - val_accuracy: 0.8395\n",
      "Epoch 23/25\n",
      "48000/48000 [==============================] - 35s 722us/sample - loss: 0.1693 - accuracy: 0.9320 - val_loss: 0.4705 - val_accuracy: 0.8427\n",
      "Epoch 24/25\n",
      "  384/48000 [..............................] - ETA: 33s - loss: 0.1384 - accuracy: 0.9453"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs = 25, batch_size = 128, verbose = 1, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 4s 625us/sample - loss: 0.3802 - accuracy: 0.8853\n",
      "Loss = 0.38016908676425615\n",
      "Test Accuracy = 0.88533336\n"
     ]
    }
   ],
   "source": [
    "preds = model.evaluate(X_test, y_test)\n",
    "print (\"Loss = \" + str(preds[0]))\n",
    "print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "\n",
    "This notebook presents the ResNet algorithm due to He et al. (2015). The implementation here also took significant inspiration and follows the structure given in the github repository of Francois Chollet: \n",
    "\n",
    "- Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun - [Deep Residual Learning for Image Recognition (2015)](https://arxiv.org/abs/1512.03385)\n",
    "- Francois Chollet's github repository: https://github.com/fchollet/deep-learning-models/blob/master/resnet50.py\n"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "OEpi5",
   "launcher_item_id": "jK9EQ"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
