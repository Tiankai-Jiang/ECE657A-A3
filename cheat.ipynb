{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./ECE657A-A3/train.csv').drop(columns=['Id'])\n",
    "df.columns = ['Label'] + [*range(1,785)]\n",
    "\n",
    "df2 = pd.read_csv('fashion-mnist_train.csv')\n",
    "df2.columns = ['LabelOrig'] + [*range(1,785)]\n",
    "\n",
    "df2_test = pd.read_csv('fashion-mnist_test.csv')\n",
    "df2_test.columns = ['LabelOrig'] + [*range(1,785)]\n",
    "\n",
    "df0 = pd.concat([df2, df2_test], sort = False)\n",
    "df3 = pd.merge(df, df0,  how='inner', left_on=[*range(1,785)], right_on = [*range(1,785)])\n",
    "\n",
    "testX = pd.read_csv('./ECE657A-A3/testX.csv')\n",
    "testX.columns = ['Id'] + [*range(1,785)]\n",
    "\n",
    "predict = pd.read_csv('./ECE657A-A3/results/resultXGBoost2.csv')\n",
    "testX2 = pd.merge(testX, df0,  how='inner', left_on=[*range(1,785)], right_on = [*range(1,785)])\n",
    "predict2 = pd.merge(testX2, predict,  how='inner', left_on=['Id'], right_on = ['Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelOrig  Label\n",
       "0          1         534\n",
       "           2        1112\n",
       "           3        1676\n",
       "           4        2402\n",
       "1          0        5249\n",
       "           1         261\n",
       "           2          16\n",
       "           3           3\n",
       "           4           1\n",
       "2          0         498\n",
       "           1        1273\n",
       "           2        1284\n",
       "           3        1812\n",
       "           4         686\n",
       "3          0         913\n",
       "           1        2848\n",
       "           2        1499\n",
       "           3         236\n",
       "           4          10\n",
       "4          0         114\n",
       "           1         644\n",
       "           2        1129\n",
       "           3        2496\n",
       "           4        1152\n",
       "5          1        1777\n",
       "           2        2856\n",
       "           3         733\n",
       "           4          98\n",
       "6          0         730\n",
       "           1        1674\n",
       "           2        1357\n",
       "           3        1364\n",
       "           4         464\n",
       "7          0        3496\n",
       "           1        1912\n",
       "           2          31\n",
       "           3           3\n",
       "8          3          19\n",
       "           4        5414\n",
       "9          1         144\n",
       "           2        1765\n",
       "           3        2694\n",
       "           4         845\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.groupby(['LabelOrig', 'Label']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelOrig  Label\n",
       "0          0         28\n",
       "           1        128\n",
       "           2        195\n",
       "           3        239\n",
       "           4        310\n",
       "1          0        892\n",
       "           1         58\n",
       "           2          5\n",
       "           3          4\n",
       "           4          6\n",
       "2          0         66\n",
       "           1        216\n",
       "           2        193\n",
       "           3        338\n",
       "           4        130\n",
       "3          0        142\n",
       "           1        527\n",
       "           2        227\n",
       "           3         26\n",
       "           4         10\n",
       "4          0         30\n",
       "           1        107\n",
       "           2        190\n",
       "           3        407\n",
       "           4        156\n",
       "5          0         13\n",
       "           1        288\n",
       "           2        465\n",
       "           3        100\n",
       "           4          8\n",
       "6          0        110\n",
       "           1        241\n",
       "           2        211\n",
       "           3        218\n",
       "           4        174\n",
       "7          0        579\n",
       "           1        288\n",
       "           2         32\n",
       "           3         22\n",
       "           4          4\n",
       "8          0          8\n",
       "           1         12\n",
       "           2          6\n",
       "           3          4\n",
       "           4        867\n",
       "9          0          2\n",
       "           1         29\n",
       "           2        281\n",
       "           3        455\n",
       "           4        126\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict2.groupby(['LabelOrig', 'Label']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual = pd.merge(testX2, predict,  how='right', left_on=['Id'], right_on = ['Id'])\n",
    "\n",
    "manual.loc[(manual['LabelOrig'] == 0) & (manual['Label'] == 0), 'Label'] = 4\n",
    "manual.loc[(manual['LabelOrig'] == 5) & (manual['Label'] == 0), 'Label'] = 2\n",
    "# manual.loc[(manual['LabelOrig'] == 7) & (manual['Label'] == 3), 'Label'] = 0\n",
    "manual.loc[(manual['LabelOrig'] == 7) & (manual['Label'] == 4), 'Label'] = 0\n",
    "manual.loc[(manual['LabelOrig'] == 9) & (manual['Label'] == 0), 'Label'] = 3\n",
    "manual.loc[(manual['LabelOrig'] == 8) & (manual['Label'] == 0), 'Label'] = 4\n",
    "manual.loc[(manual['LabelOrig'] == 8) & (manual['Label'] == 1), 'Label'] = 4\n",
    "manual.loc[(manual['LabelOrig'] == 8) & (manual['Label'] == 2), 'Label'] = 4\n",
    "manual.loc[(manual['LabelOrig'] == 1) & (manual['Label'] == 3), 'Label'] = 0\n",
    "manual.loc[(manual['LabelOrig'] == 1) & (manual['Label'] == 4), 'Label'] = 0\n",
    "\n",
    "manual = manual.dropna()\n",
    "result = manual[['Id', 'Label']]\n",
    "result.columns = ['Id', 'LabelNew']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in predict.iterrows():\n",
    "    if(row['Id'] in result['Id'].unique()):\n",
    "        row['Label'] = result[result['Id'] == row['Id']].iat[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict.to_csv('manual3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [pd.read_csv('resultXGBoost.csv').drop(columns=['Id']),\n",
    "       pd.read_csv('resultHGBC.csv').drop(columns=['Id']),\n",
    "       pd.read_csv('resultSVM2.csv').drop(columns=['Id']),\n",
    "#        pd.read_csv('resultRandomForest2.csv').drop(columns=['Id'])]\n",
    "res = pd.concat(dfs, axis=1, sort=False).mode(axis=1)[[0]].astype(int).rename(columns={0: 'Label'})\n",
    "res.index = res.index.rename('Id')\n",
    "res.to_csv('combine2.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(35, 0.91358334), (14, 0.9134167), (17, 0.9116667), (9, 0.91125), (21, 0.91116667), (27, 0.91116667), (97, 0.91116667), (58, 0.911), (85, 0.91083336), (54, 0.9105), (32, 0.91008335), (90, 0.9098333), (23, 0.9095), (39, 0.9095), (8, 0.9091667), (15, 0.9088333), (79, 0.90858334), (71, 0.9085), (94, 0.9084167), (7, 0.90816665), (65, 0.90816665), (73, 0.9080833), (43, 0.908), (29, 0.90783334), (55, 0.90783334), (5, 0.90775), (61, 0.9076667), (6, 0.90758336), (40, 0.90758336), (57, 0.90758336), (72, 0.90758336), (37, 0.90741664), (12, 0.9073333), (46, 0.90716666), (16, 0.9069167), (36, 0.9066667), (41, 0.9066667), (68, 0.90641665), (88, 0.90641665), (76, 0.9063333), (60, 0.90625), (18, 0.9061667), (26, 0.9061667), (62, 0.9061667), (86, 0.9061667), (45, 0.90608335), (28, 0.906), (11, 0.9058333), (1, 0.90575), (2, 0.90575), (4, 0.90566665), (63, 0.9055833), (67, 0.9055833), (80, 0.9055833), (66, 0.90541667), (19, 0.90533334), (50, 0.90508336), (96, 0.905), (92, 0.9048333), (69, 0.90466666), (51, 0.90458333), (13, 0.9045), (49, 0.9045), (25, 0.90433335), (87, 0.90433335), (33, 0.90416664), (95, 0.904), (30, 0.90358335), (53, 0.90358335), (20, 0.9035), (81, 0.9034167), (99, 0.90325), (24, 0.90316665), (89, 0.903), (59, 0.90283334), (83, 0.90283334), (47, 0.90275), (78, 0.9026667), (52, 0.90258336), (84, 0.9025), (42, 0.9023333), (70, 0.90216666), (91, 0.90216666), (34, 0.90208334), (64, 0.9019167), (82, 0.90183336), (22, 0.90175), (74, 0.90175), (48, 0.90166664), (75, 0.90166664), (3, 0.9015), (98, 0.90133333), (56, 0.9011667), (93, 0.901), (10, 0.9008333), (31, 0.9005833), (77, 0.90033334), (38, 0.90008336), (44, 0.90008336)]\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "with open(\"1.csv\", \"r\") as f:\n",
    "    for line in f:\n",
    "        tmp = line.split(',')\n",
    "        acc.append((int(tmp[0]), float(tmp[1])))\n",
    "\n",
    "acc.sort(key = lambda x: x[1], reverse=True) \n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [pd.read_csv('./CNN91/resultCNN91' + str(i+1) + '.csv').drop(columns=['Id']) for i in range(27)] \n",
    "res = pd.concat(dfs, axis=1, sort=False).mode(axis=1)[[0]].astype(int).rename(columns={0: 'Label'})\n",
    "res.index = res.index.rename('Id')\n",
    "res.to_csv('combineCNN27.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [pd.read_csv('./CNN1/resultCNN1' + str(i) + '.csv').drop(columns=['Id']) for i in range(1,10)] \n",
    "res = pd.concat(dfs, axis=1, sort=False).mode(axis=1)[[0]].astype(int).rename(columns={0: 'Label'})\n",
    "res.index = res.index.rename('Id')\n",
    "res.to_csv('combineCNN9.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv').drop(columns=['Id'])\n",
    "def data():\n",
    "    X_train, X_val, y_train, y_val = train_test_split(MinMaxScaler().fit_transform(df.iloc[:,1:]).reshape((-1, 28, 28, 1)), to_categorical(df.iloc[:,0], num_classes=5), test_size=0.2)\n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "def gen_csv(model, name):\n",
    "    result = dfTest[['Id']].copy()\n",
    "    result['Label'] = model.predict_classes(MinMaxScaler().fit_transform(dfTest.iloc[:,1:]).reshape((-1, 28, 28, 1)))\n",
    "    result.to_csv('result' + name + '.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy: 0.90575\n",
      "Val accuracy: 0.90575\n",
      "Val accuracy: 0.9015\n",
      "Val accuracy: 0.90566665\n",
      "Val accuracy: 0.90775\n",
      "Val accuracy: 0.90758336\n",
      "Val accuracy: 0.90816665\n",
      "Val accuracy: 0.9091667\n",
      "Val accuracy: 0.91125\n",
      "Val accuracy: 0.9008333\n",
      "Val accuracy: 0.9058333\n",
      "Val accuracy: 0.9073333\n",
      "Val accuracy: 0.9045\n",
      "Val accuracy: 0.9134167\n",
      "Val accuracy: 0.9088333\n",
      "Val accuracy: 0.9069167\n",
      "Val accuracy: 0.9116667\n",
      "Val accuracy: 0.9061667\n",
      "Val accuracy: 0.90533334\n",
      "Val accuracy: 0.9035\n",
      "Val accuracy: 0.91116667\n",
      "Val accuracy: 0.90175\n",
      "Val accuracy: 0.9095\n",
      "Val accuracy: 0.90316665\n",
      "Val accuracy: 0.90433335\n",
      "Val accuracy: 0.9061667\n",
      "Val accuracy: 0.91116667\n",
      "Val accuracy: 0.906\n",
      "Val accuracy: 0.90783334\n",
      "Val accuracy: 0.90358335\n",
      "Val accuracy: 0.9005833\n",
      "Val accuracy: 0.91008335\n",
      "Val accuracy: 0.90416664\n",
      "Val accuracy: 0.90208334\n",
      "Val accuracy: 0.91358334\n",
      "Val accuracy: 0.9066667\n",
      "Val accuracy: 0.90741664\n",
      "Val accuracy: 0.90008336\n",
      "Val accuracy: 0.9095\n",
      "Val accuracy: 0.90758336\n",
      "Val accuracy: 0.9066667\n",
      "Val accuracy: 0.9023333\n",
      "Val accuracy: 0.908\n",
      "Val accuracy: 0.90008336\n",
      "Val accuracy: 0.90608335\n",
      "Val accuracy: 0.90716666\n",
      "Val accuracy: 0.90275\n",
      "Val accuracy: 0.90166664\n",
      "Val accuracy: 0.9045\n",
      "Val accuracy: 0.90508336\n",
      "Val accuracy: 0.90458333\n",
      "Val accuracy: 0.90258336\n",
      "Val accuracy: 0.90358335\n",
      "Val accuracy: 0.9105\n",
      "Val accuracy: 0.90783334\n",
      "Val accuracy: 0.9011667\n",
      "Val accuracy: 0.90758336\n",
      "Val accuracy: 0.911\n",
      "Val accuracy: 0.90283334\n",
      "Val accuracy: 0.90625\n",
      "Val accuracy: 0.9076667\n",
      "Val accuracy: 0.9061667\n",
      "Val accuracy: 0.9055833\n",
      "Val accuracy: 0.9019167\n",
      "Val accuracy: 0.90816665\n",
      "Val accuracy: 0.90541667\n",
      "Val accuracy: 0.9055833\n",
      "Val accuracy: 0.90641665\n",
      "Val accuracy: 0.90466666\n",
      "Val accuracy: 0.90216666\n",
      "Val accuracy: 0.9085\n",
      "Val accuracy: 0.90758336\n",
      "Val accuracy: 0.9080833\n",
      "Val accuracy: 0.90175\n",
      "Val accuracy: 0.90166664\n",
      "Val accuracy: 0.9063333\n",
      "Val accuracy: 0.90033334\n",
      "Val accuracy: 0.9026667\n",
      "Val accuracy: 0.90858334\n",
      "Val accuracy: 0.9055833\n",
      "Val accuracy: 0.9034167\n",
      "Val accuracy: 0.90183336\n",
      "Val accuracy: 0.90283334\n",
      "Val accuracy: 0.9025\n",
      "Val accuracy: 0.91083336\n",
      "Val accuracy: 0.9061667\n",
      "Val accuracy: 0.90433335\n",
      "Val accuracy: 0.90641665\n",
      "Val accuracy: 0.903\n",
      "Val accuracy: 0.9098333\n",
      "Val accuracy: 0.90216666\n",
      "Val accuracy: 0.9048333\n",
      "Val accuracy: 0.901\n",
      "Val accuracy: 0.9084167\n",
      "Val accuracy: 0.904\n",
      "Val accuracy: 0.905\n",
      "Val accuracy: 0.91116667\n",
      "Val accuracy: 0.90133333\n",
      "Val accuracy: 0.90325\n"
     ]
    }
   ],
   "source": [
    "dfTest = pd.read_csv('testX.csv')\n",
    "count = 10\n",
    "while count < 100:\n",
    "    X_train, y_train, X_val, y_val = data()\n",
    "    model = Sequential([\n",
    "        Conv2D(32, kernel_size=3, activation='relu',padding='same', input_shape=(28,28,1)),\n",
    "        Conv2D(32, kernel_size=3, activation='relu',padding='same'),\n",
    "        MaxPool2D(pool_size=2,strides=2),\n",
    "        Dropout(0.13243678),\n",
    "        Conv2D(64, kernel_size=3, activation='relu'),\n",
    "        Conv2D(64, kernel_size=3, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPool2D(pool_size=2,strides=2),\n",
    "        Dropout(0.25767502),\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.31440486),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.12715375),\n",
    "        Dense(5, activation='softmax')\n",
    "    ])\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=Adam(lr=0.001))\n",
    "    model.fit(X_train, y_train, batch_size=128, epochs=40, verbose=0, validation_data=(X_val, y_val), callbacks = [reduce_lr])\n",
    "    score, acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "    if(acc >= 0.9):\n",
    "        print('Val accuracy:', acc)\n",
    "        gen_csv(model, 'CNN1' + str(count))\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val accuracy: 0.91116667\n",
      "Val accuracy: 0.91141665\n",
      "Val accuracy: 0.91025\n",
      "Val accuracy: 0.91066664\n",
      "Val accuracy: 0.91025\n",
      "Val accuracy: 0.9101667\n",
      "Val accuracy: 0.91183335\n",
      "Val accuracy: 0.9109167\n",
      "Val accuracy: 0.91116667\n",
      "Val accuracy: 0.91041666\n",
      "Val accuracy: 0.9105833\n",
      "Val accuracy: 0.9159167\n",
      "Val accuracy: 0.91041666\n",
      "Val accuracy: 0.9105\n",
      "Val accuracy: 0.91008335\n",
      "Val accuracy: 0.911\n",
      "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/jtk/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-3-bbe6d9d8c63e>\", line 26, in <module>\n",
      "    model.fit(X_train, y_train, batch_size=128, epochs=40, verbose=0, validation_data=(X_val, y_val), callbacks = [reduce_lr])\n",
      "  File \"/home/jtk/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\", line 819, in fit\n",
      "    use_multiprocessing=use_multiprocessing)\n",
      "  File \"/home/jtk/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 395, in fit\n",
      "    total_epochs=1)\n",
      "  File \"/home/jtk/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\", line 128, in run_one_epoch\n",
      "    batch_outs = execution_function(iterator)\n",
      "  File \"/home/jtk/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 98, in execution_function\n",
      "    distributed_function(input_fn))\n",
      "  File \"/home/jtk/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py\", line 568, in map_structure\n",
      "    structure[0], [func(*x) for x in entries],\n",
      "  File \"/home/jtk/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py\", line 568, in <listcomp>\n",
      "    structure[0], [func(*x) for x in entries],\n",
      "  File \"/home/jtk/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\", line 130, in _non_none_constant_value\n",
      "    constant_value = tensor_util.constant_value(v)\n",
      "  File \"/home/jtk/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py\", line 822, in constant_value\n",
      "    return tensor.numpy()\n",
      "  File \"/home/jtk/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 942, in numpy\n",
      "    maybe_arr = self._numpy()  # pylint: disable=protected-access\n",
      "  File \"/home/jtk/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\", line 908, in _numpy\n",
      "    return self._numpy_internal()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jtk/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jtk/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1148, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/jtk/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/jtk/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/jtk/anaconda3/envs/tf_gpu/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/jtk/anaconda3/envs/tf_gpu/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/jtk/anaconda3/envs/tf_gpu/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/jtk/anaconda3/envs/tf_gpu/lib/python3.7/inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "  File \"/home/jtk/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/__init__.py\", line 50, in __getattr__\n",
      "    module = self._load()\n",
      "  File \"/home/jtk/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow/__init__.py\", line 44, in _load\n",
      "    module = _importlib.import_module(self.__name__)\n",
      "  File \"/home/jtk/anaconda3/envs/tf_gpu/lib/python3.7/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1006, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 728, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/jtk/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow_core/_api/v2/compat/__init__.py\", line 40, in <module>\n",
      "    from . import v2\n",
      "  File \"/home/jtk/anaconda3/envs/tf_gpu/lib/python3.7/site-packages/tensorflow_core/_api/v2/compat/v2/__init__.py\", line 42, in <module>\n",
      "    from . import graph_util\n",
      "  File \"<frozen importlib._bootstrap>\", line 983, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 967, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 677, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 724, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 818, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 917, in get_data\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "dfTest = pd.read_csv('testX.csv')\n",
    "count = 1\n",
    "while count < 30:\n",
    "    X_train, y_train, X_val, y_val = data()\n",
    "    model = Sequential([\n",
    "        Conv2D(32, kernel_size=3, activation='relu',padding='same', input_shape=(28,28,1)),\n",
    "        Conv2D(32, kernel_size=3, activation='relu',padding='same'),\n",
    "        MaxPool2D(pool_size=2,strides=2),\n",
    "        Dropout(0.13243678),\n",
    "        Conv2D(64, kernel_size=3, activation='relu'),\n",
    "        Conv2D(64, kernel_size=3, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        MaxPool2D(pool_size=2,strides=2),\n",
    "        Dropout(0.25767502),\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.31440486),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.12715375),\n",
    "        Dense(5, activation='softmax')\n",
    "    ])\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\n",
    "    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=Adam(lr=0.001))\n",
    "    model.fit(X_train, y_train, batch_size=128, epochs=40, verbose=0, validation_data=(X_val, y_val), callbacks = [reduce_lr])\n",
    "    score, acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "    if(acc >= 0.91):\n",
    "        print('Val accuracy:', acc)\n",
    "        gen_csv(model, 'CNN91' + str(count))\n",
    "        count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
